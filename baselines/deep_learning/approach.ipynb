{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4764d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from Feature_extract import feature_transform\n",
    "from Datagenerator import Datagen\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Model import ProtoNet,ResNet\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from batch_sampler import EpisodicBatchSampler\n",
    "from torch.nn import functional as F\n",
    "from util import prototypical_loss as loss_fn\n",
    "from util import evaluate_prototypes\n",
    "from glob import glob\n",
    "import hydra\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import pathlib\n",
    "import pprint\n",
    "import SepTr\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "config_dir = pathlib.Path('.')\n",
    "hydra.initialize(config_path=config_dir)\n",
    "\n",
    "conf = hydra.compose(config_name='config_online.yaml', overrides=[\"set.train=True\",])\n",
    "\n",
    "def init_seed():\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b8957",
   "metadata": {},
   "source": [
    "# In this notebook we will\n",
    "- make a dataloader which loads raw audio\n",
    "- apply transformations to the audio\n",
    "- make dataloader which uses transformations\n",
    "- ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ea262",
   "metadata": {},
   "source": [
    "# Load balanced raw audio dataset\n",
    "- how to load audio\n",
    "- how to balance it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8143b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(df_pos,pcen,glob_cls_name,file_name,hf,seg_len,hop_seg,fps):\n",
    "\n",
    "    '''Chunk the time-frequecy representation to segment length and store in h5py dataset\n",
    "\n",
    "    Args:\n",
    "        -df_pos : dataframe\n",
    "        -log_mel_spec : log mel spectrogram\n",
    "        -glob_cls_name: Name of the class used in audio files where only one class is present\n",
    "        -file_name : Name of the csv file\n",
    "        -hf: h5py object\n",
    "        -seg_len : fixed segment length\n",
    "        -fps: frame per second\n",
    "    Out:\n",
    "        - label_list: list of labels for the extracted mel patches'''\n",
    "\n",
    "    label_list = []\n",
    "    if len(hf['features'][:]) == 0:\n",
    "        file_index = 0\n",
    "    else:\n",
    "        file_index = len(hf['features'][:])\n",
    "\n",
    "\n",
    "    start_time,end_time = time_2_frame(df_pos,fps)\n",
    "\n",
    "\n",
    "    'For csv files with a column name Call, pick up the global class name'\n",
    "\n",
    "    if 'CALL' in df_pos.columns:\n",
    "        cls_list = [glob_cls_name] * len(start_time)\n",
    "    else:\n",
    "        cls_list = [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, row in df_pos.iterrows()]\n",
    "        cls_list = list(chain.from_iterable(cls_list))\n",
    "\n",
    "    assert len(start_time) == len(end_time)\n",
    "    assert len(cls_list) == len(start_time)\n",
    "\n",
    "    for index in range(len(start_time)):\n",
    "\n",
    "        str_ind = start_time[index]\n",
    "        end_ind = end_time[index]\n",
    "        label = cls_list[index]\n",
    "\n",
    "        'Extract segment and move forward with hop_seg'\n",
    "\n",
    "        if end_ind - str_ind > seg_len:\n",
    "            shift = 0\n",
    "            while end_ind - (str_ind + shift) > seg_len:\n",
    "\n",
    "                pcen_patch = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "\n",
    "                hf['features'].resize((file_index + 1, pcen_patch.shape[0], pcen_patch.shape[1]))\n",
    "                hf['features'][file_index] = pcen_patch\n",
    "                label_list.append(label)\n",
    "                file_index += 1\n",
    "                shift = shift + hop_seg\n",
    "\n",
    "            pcen_patch_last = pcen[end_ind - seg_len:end_ind]\n",
    "\n",
    "\n",
    "\n",
    "            hf['features'].resize((file_index+1 , pcen_patch.shape[0], pcen_patch.shape[1]))\n",
    "            hf['features'][file_index] = pcen_patch_last\n",
    "            label_list.append(label)\n",
    "            file_index += 1\n",
    "        else:\n",
    "\n",
    "            'If patch length is less than segment length then tile the patch multiple times till it reaches the segment length'\n",
    "\n",
    "            pcen_patch = pcen[str_ind:end_ind]\n",
    "            if pcen_patch.shape[0] == 0:\n",
    "                print(pcen_patch.shape[0])\n",
    "                print(\"The patch is of 0 length\")\n",
    "                continue\n",
    "\n",
    "            repeat_num = int(seg_len / (pcen_patch.shape[0])) + 1\n",
    "            pcen_patch_new = np.tile(pcen_patch, (repeat_num, 1))\n",
    "            pcen_patch_new = pcen_patch_new[0:int(seg_len)]\n",
    "            hf['features'].resize((file_index+1, pcen_patch_new.shape[0], pcen_patch_new.shape[1]))\n",
    "            hf['features'][file_index] = pcen_patch_new\n",
    "            label_list.append(label)\n",
    "            file_index += 1\n",
    "\n",
    "    \n",
    "    print(\"Total files created : {}\".format(file_index))\n",
    "    return label_list\n",
    "\n",
    "class Feature_Extractor():\n",
    "\n",
    "       def __init__(self, conf):\n",
    "           self.sr =conf.features.sr\n",
    "           self.n_fft = conf.features.n_fft\n",
    "           self.hop = conf.features.hop_mel\n",
    "           self.n_mels = conf.features.n_mels\n",
    "           self.fmax = conf.features.fmax\n",
    "           #self.win_length = conf.features.win_length\n",
    "       def extract_feature(self,audio):\n",
    "\n",
    "           mel_spec = librosa.feature.melspectrogram(audio,sr=self.sr, n_fft=self.n_fft,\n",
    "                                                     hop_length=self.hop,n_mels=self.n_mels,fmax=self.fmax)\n",
    "           pcen = librosa.core.pcen(mel_spec,sr=22050)\n",
    "           pcen = pcen.astype(np.float32)\n",
    "\n",
    "           return pcen\n",
    "\n",
    "def extract_feature(audio_path,feat_extractor,conf):\n",
    "\n",
    "    y,fs = librosa.load(audio_path,sr=conf.features.sr)\n",
    "\n",
    "    'Scaling audio as per suggestion in librosa documentation'\n",
    "\n",
    "    y = y * (2**32)\n",
    "    pcen = feat_extractor.extract_feature(y)\n",
    "    return pcen.T\n",
    "\n",
    "\n",
    "\n",
    "def time_2_frame(df,fps):\n",
    "\n",
    "\n",
    "    'Margin of 25 ms around the onset and offsets'\n",
    "\n",
    "    df.loc[:,'Starttime'] = df['Starttime'] - 0.025\n",
    "    df.loc[:,'Endtime'] = df['Endtime'] + 0.025\n",
    "\n",
    "    'Converting time to frames'\n",
    "\n",
    "    start_time = [int(np.floor(start * fps)) for start in df['Starttime']]\n",
    "\n",
    "    end_time = [int(np.floor(end * fps)) for end in df['Endtime']]\n",
    "\n",
    "    return start_time,end_time\n",
    "\n",
    "def feature_transform(conf=None,mode=None):\n",
    "    '''\n",
    "       Training:\n",
    "          Extract mel-spectrogram/PCEN and slice each data sample into segments of length conf.seg_len.\n",
    "          Each segment inherits clip level label. The segment length is kept same across training\n",
    "          and validation set.\n",
    "       Evaluation:\n",
    "           Currently using the validation set for evaluation.\n",
    "           \n",
    "           For each audio file, extract time-frequency representation and create 3 subsets:\n",
    "           a) Positive set - Extract segments based on the provided onset-offset annotations.\n",
    "           b) Negative set - Since there is no negative annotation provided, we consider the entire\n",
    "                         audio file as the negative class and extract patches of length conf.seg_len\n",
    "           c) Query set - From the end time of the 5th annotation to the end of the audio file.\n",
    "                          Onset-offset prediction is made on this subset.\n",
    "\n",
    "       Args:\n",
    "       - config: config object\n",
    "       - mode: train/valid\n",
    "\n",
    "       Out:\n",
    "       - Num_extract_train/Num_extract_valid - Number of samples in training/validation set\n",
    "                                                                                              '''\n",
    "\n",
    "\n",
    "    label_tr = []\n",
    "    pcen_extractor = Feature_Extractor(conf)\n",
    "\n",
    "    fps =  conf.features.sr / conf.features.hop_mel\n",
    "    'Converting fixed segment legnth to frames'\n",
    "\n",
    "    seg_len = int(round(conf.features.seg_len * fps))\n",
    "    hop_seg = int(round(conf.features.hop_seg * fps))\n",
    "    extension = \"*.csv\"\n",
    "\n",
    "\n",
    "    if mode == 'train':\n",
    "\n",
    "        print(\"=== Processing training set ===\")\n",
    "        meta_path = conf.path.train_dir\n",
    "        all_csv_files = [file\n",
    "                         for path_dir, subdir, files in os.walk(meta_path)\n",
    "                         for file in glob(os.path.join(path_dir, extension))]\n",
    "        all_csv_files = all_csv_files[:100]\n",
    "        hdf_tr = os.path.join(conf.path.feat_train,'Mel_train.h5')\n",
    "        hf = h5py.File(hdf_tr,'w')\n",
    "        hf.create_dataset('features', shape=(0, seg_len, conf.features.n_mels),\n",
    "                          maxshape=(None, seg_len, conf.features.n_mels))\n",
    "        num_extract = 0\n",
    "        for file in all_csv_files:\n",
    "\n",
    "            split_list = file.split('/')\n",
    "            glob_cls_name = split_list[split_list.index('Training_Set') + 1]\n",
    "            file_name = split_list[split_list.index('Training_Set') + 2]\n",
    "            df = pd.read_csv(file, header=0, index_col=False)\n",
    "            audio_path = file.replace('csv', 'wav')\n",
    "            print(\"Processing file name {}\".format(audio_path))\n",
    "            pcen = extract_feature(audio_path, pcen_extractor,conf)\n",
    "            df_pos = df[(df == 'POS').any(axis=1)]\n",
    "            label_list = create_dataset(df_pos,pcen,glob_cls_name,file_name,hf,seg_len,hop_seg,fps)\n",
    "            label_tr.append(label_list)\n",
    "        print(\" Feature extraction for training set complete\")\n",
    "        num_extract = len(hf['features'])\n",
    "        flat_list = [item for sublist in label_tr for item in sublist]\n",
    "        hf.create_dataset('labels', data=[s.encode() for s in flat_list], dtype='S20')\n",
    "        data_shape = hf['features'].shape\n",
    "        hf.close()\n",
    "        return num_extract,data_shape\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"=== Processing Validation set ===\")\n",
    "\n",
    "        meta_path = conf.path.eval_dir\n",
    "\n",
    "        all_csv_files = [file\n",
    "                         for path_dir, subdir, files in os.walk(meta_path)\n",
    "                         for file in glob(os.path.join(path_dir, extension))]\n",
    "\n",
    "        num_extract_eval = 0\n",
    "\n",
    "        for file in all_csv_files:\n",
    "\n",
    "            idx_pos = 0\n",
    "            idx_neg = 0\n",
    "            start_neg = 0\n",
    "            hop_neg = 0\n",
    "            idx_query = 0\n",
    "            hop_query = 0\n",
    "            strt_index = 0\n",
    "\n",
    "            split_list = file.split('/')\n",
    "            name = str(split_list[-1].split('.')[0])\n",
    "            feat_name = name + '.h5'\n",
    "            audio_path = file.replace('csv', 'wav')\n",
    "            feat_info = []\n",
    "            hdf_eval = os.path.join(conf.path.feat_eval,feat_name)\n",
    "            hf = h5py.File(hdf_eval,'w')\n",
    "            \n",
    "\n",
    "            df_eval = pd.read_csv(file, header=0, index_col=False)\n",
    "            Q_list = df_eval['Q'].to_numpy()\n",
    "\n",
    "            start_time,end_time = time_2_frame(df_eval,fps)\n",
    "\n",
    "            index_sup = np.where(Q_list == 'POS')[0][:conf.train.n_shot]\n",
    "\n",
    "            difference = []\n",
    "            for index in index_sup:\n",
    "                difference.append(end_time[index] - start_time[index])\n",
    "            \n",
    "            # Adaptive segment length based on the audio file. \n",
    "            max_len = max(difference)\n",
    "            \n",
    "            # Choosing the segment length based on the maximum size in the 5-shot.\n",
    "            # Logic was based on fitment on 12GB GPU since some segments are quite long. \n",
    "            if max_len < 100:\n",
    "\n",
    "                seg_len = max_len\n",
    "            elif max_len > 100 and max_len < 500 :\n",
    "                seg_len = max_len//4\n",
    "            else:\n",
    "                seg_len = max_len//8\n",
    "                \n",
    "\n",
    "            \n",
    "            print(f\"Segment length for file is {seg_len}\")\n",
    "            hop_seg = seg_len//2\n",
    "\n",
    "            hf.create_dataset('feat_pos', shape=(0, seg_len, conf.features.n_mels),\n",
    "                              maxshape= (None, seg_len, conf.features.n_mels))\n",
    "            hf.create_dataset('feat_query',shape=(0,seg_len,conf.features.n_mels),maxshape=(None,seg_len,conf.features.n_mels))\n",
    "            hf.create_dataset('feat_neg',shape=(0,seg_len,conf.features.n_mels),maxshape=(None,seg_len,conf.features.n_mels))\n",
    "            hf.create_dataset('start_index_query',shape=(1,),maxshape=(None))\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            hf.create_dataset('seg_len',shape=(1,), maxshape=(None))\n",
    "            hf.create_dataset('hop_seg',shape=(1,), maxshape=(None))\n",
    "            pcen = extract_feature(audio_path, pcen_extractor,conf)\n",
    "            mean = np.mean(pcen)\n",
    "            std = np.mean(pcen)\n",
    "            hf['seg_len'][:] = seg_len\n",
    "            hf['hop_seg'][:] = hop_seg\n",
    "\n",
    "            strt_indx_query = end_time[index_sup[-1]]\n",
    "            end_idx_neg = pcen.shape[0] - 1\n",
    "            hf['start_index_query'][:] = strt_indx_query\n",
    "\n",
    "            print(\"Creating negative dataset\")\n",
    "\n",
    "            while end_idx_neg - (strt_index + hop_neg) > seg_len:\n",
    "\n",
    "                patch_neg = pcen[int(strt_index + hop_neg):int(strt_index + hop_neg + seg_len)]\n",
    "\n",
    "                hf['feat_neg'].resize((idx_neg + 1, patch_neg.shape[0], patch_neg.shape[1]))\n",
    "                hf['feat_neg'][idx_neg] = patch_neg\n",
    "                idx_neg += 1\n",
    "                hop_neg += hop_seg\n",
    "\n",
    "            last_patch = pcen[end_idx_neg - seg_len:end_idx_neg]\n",
    "            hf['feat_neg'].resize((idx_neg + 1, last_patch.shape[0], last_patch.shape[1]))\n",
    "            hf['feat_neg'][idx_neg] = last_patch\n",
    "\n",
    "            print(\"Creating Positive dataset\")\n",
    "            for index in index_sup:\n",
    "\n",
    "                str_ind = int(start_time[index])\n",
    "                end_ind = int(end_time[index])\n",
    "\n",
    "                if end_ind - str_ind > seg_len:\n",
    "\n",
    "                    shift = 0\n",
    "                    while end_ind - (str_ind + shift) > seg_len:\n",
    "\n",
    "                        patch_pos = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "\n",
    "                        hf['feat_pos'].resize((idx_pos + 1, patch_pos.shape[0], patch_pos.shape[1]))\n",
    "                        hf['feat_pos'][idx_pos] = patch_pos\n",
    "                        idx_pos += 1\n",
    "                        shift += hop_seg\n",
    "                    last_patch_pos = pcen[end_ind - seg_len:end_ind]\n",
    "                    hf['feat_pos'].resize((idx_pos + 1, patch_pos.shape[0], patch_pos.shape[1]))\n",
    "                    hf['feat_pos'][idx_pos] = last_patch_pos\n",
    "                    idx_pos += 1\n",
    "\n",
    "                else:\n",
    "                    patch_pos = pcen[str_ind:end_ind]\n",
    "\n",
    "                    if patch_pos.shape[0] == 0:\n",
    "                        print(patch_pos.shape[0])\n",
    "                        print(\"The patch is of 0 length\")\n",
    "                        continue\n",
    "                    repeat_num = int(seg_len / (patch_pos.shape[0])) + 1\n",
    "\n",
    "                    patch_new = np.tile(patch_pos, (repeat_num, 1))\n",
    "                    patch_new = patch_new[0:int(seg_len)]\n",
    "                    hf['feat_pos'].resize((idx_pos + 1, patch_new.shape[0], patch_new.shape[1]))\n",
    "                    hf['feat_pos'][idx_pos] = patch_new\n",
    "                    idx_pos += 1\n",
    "\n",
    "\n",
    "\n",
    "            print(\"Creating query dataset\")\n",
    "\n",
    "            while end_idx_neg - (strt_indx_query + hop_query) > seg_len:\n",
    "\n",
    "                patch_query = pcen[int(strt_indx_query + hop_query):int(strt_indx_query + hop_query + seg_len)]\n",
    "                hf['feat_query'].resize((idx_query + 1, patch_query.shape[0], patch_query.shape[1]))\n",
    "                hf['feat_query'][idx_query] = patch_query\n",
    "                idx_query += 1\n",
    "                hop_query += hop_seg\n",
    "\n",
    "\n",
    "            last_patch_query = pcen[end_idx_neg - seg_len:end_idx_neg]\n",
    "\n",
    "            hf['feat_query'].resize((idx_query + 1, last_patch_query.shape[0], last_patch_query.shape[1]))\n",
    "            hf['feat_query'][idx_query] = last_patch_query\n",
    "            num_extract_eval += len(hf['feat_query'])\n",
    "\n",
    "            hf.close()\n",
    "\n",
    "        return num_extract_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6362060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk up the original and store in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8247cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_protonet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m ProtoNet()\n\u001b[0;32m---> 44\u001b[0m best_acc,model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_protonet\u001b[49m(encoder,train_loader,valid_loader,conf,num_episodes_tr,num_episodes_vd)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest accuracy of the model on training set is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(best_acc))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_protonet' is not defined"
     ]
    }
   ],
   "source": [
    "# training\n",
    "if conf.set.train:\n",
    "    if not os.path.isdir(conf.path.Model):\n",
    "        os.makedirs(conf.path.Model)\n",
    "\n",
    "    init_seed()\n",
    "\n",
    "    gen_train = Datagen(conf)\n",
    "    X_train,Y_train,X_val,Y_val = gen_train.generate_train()\n",
    "    X_tr = torch.tensor(X_train)\n",
    "    Y_tr = torch.LongTensor(Y_train)\n",
    "    X_val = torch.tensor(X_val)\n",
    "    Y_val = torch.LongTensor(Y_val)\n",
    "\n",
    "    samples_per_cls =  conf.train.n_shot * 2\n",
    "\n",
    "    batch_size_tr = samples_per_cls * conf.train.k_way\n",
    "    batch_size_vd = batch_size_tr\n",
    "\n",
    "    if conf.train.num_episodes is not None:\n",
    "\n",
    "        num_episodes_tr = conf.train.num_episodes\n",
    "        num_episodes_vd = conf.train.num_episodes\n",
    "\n",
    "    else:\n",
    "\n",
    "        num_episodes_tr = len(Y_train)//batch_size_tr\n",
    "        num_episodes_vd = len(Y_val)//batch_size_vd\n",
    "\n",
    "    samplr_train = EpisodicBatchSampler(Y_train,num_episodes_tr,conf.train.k_way,samples_per_cls)\n",
    "    samplr_valid = EpisodicBatchSampler(Y_val,num_episodes_vd,conf.train.k_way,samples_per_cls)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_tr,Y_tr)\n",
    "    valid_dataset = torch.utils.data.TensorDataset(X_val,Y_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_sampler=samplr_train,num_workers=0,pin_memory=True,shuffle=False)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_sampler=samplr_valid,num_workers=0,pin_memory=True,shuffle=False)\n",
    "    if conf.train.encoder == 'Resnet':\n",
    "        encoder  = ResNet()\n",
    "    else:\n",
    "        encoder = ProtoNet()\n",
    "\n",
    "\n",
    "    best_acc,model = train_protonet(encoder,train_loader,valid_loader,conf,num_episodes_tr,num_episodes_vd)\n",
    "    print(\"Best accuracy of the model on training set is {}\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a3838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_protonet(encoder,train_loader,valid_loader,conf,num_batches_tr,num_batches_vd):\n",
    "\n",
    "    '''Model training\n",
    "    Args:\n",
    "    -model: Model\n",
    "    -train_laoder: Training loader\n",
    "    -valid_load: Valid loader\n",
    "    -conf: configuration object\n",
    "    -num_batches_tr: number of training batches\n",
    "    -num_batches_vd: Number of validation batches\n",
    "    Out:\n",
    "    -best_val_acc: Best validation accuracy\n",
    "    -model\n",
    "    -best_state: State dictionary for the best validation accuracy\n",
    "    '''\n",
    "\n",
    "    if conf.train.device == 'cuda':\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    \n",
    "    optim = torch.optim.Adam([{'params':encoder.parameters()}] ,lr=conf.train.lr_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim, gamma=conf.train.scheduler_gamma,\n",
    "                                                   step_size=conf.train.scheduler_step_size)\n",
    "    num_epochs = conf.train.epochs\n",
    "\n",
    "    best_model_path = conf.path.best_model\n",
    "    last_model_path = conf.path.last_model\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    best_val_acc = 0.0\n",
    "    encoder.to(device)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        train_iterator = iter(train_loader)\n",
    "        for batch in tqdm(train_iterator):\n",
    "            optim.zero_grad()\n",
    "            encoder.train()\n",
    "            \n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x_out = encoder(x)\n",
    "            tr_loss,tr_acc = loss_fn(x_out,y,conf.train.n_shot)\n",
    "            train_loss.append(tr_loss.item())\n",
    "            train_acc.append(tr_acc.item())\n",
    "\n",
    "            tr_loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        avg_loss_tr = np.mean(train_loss[-num_batches_tr:])\n",
    "        avg_acc_tr = np.mean(train_acc[-num_batches_tr:])\n",
    "        print('Average train loss: {}  Average training accuracy: {}'.format(avg_loss_tr,avg_acc_tr))\n",
    "        lr_scheduler.step()\n",
    "        encoder.eval()\n",
    "        \n",
    "        val_iterator = iter(valid_loader)\n",
    "\n",
    "        for batch in tqdm(val_iterator):\n",
    "            x,y = batch\n",
    "            x = x.to(device)\n",
    "            x_val = encoder(x)\n",
    "            valid_loss, valid_acc = loss_fn(x_val, y, conf.train.n_shot)\n",
    "            val_loss.append(valid_loss.item())\n",
    "            val_acc.append(valid_acc.item())\n",
    "        avg_loss_vd = np.mean(val_loss[-num_batches_vd:])\n",
    "        avg_acc_vd = np.mean(val_acc[-num_batches_vd:])\n",
    "\n",
    "        print ('Epoch {}, Validation loss {:.4f}, Validation accuracy {:.4f}'.format(epoch,avg_loss_vd,avg_acc_vd))\n",
    "        if avg_acc_vd > best_val_acc:\n",
    "            print(\"Saving the best model with valdation accuracy {}\".format(avg_acc_vd))\n",
    "            best_val_acc = avg_acc_vd\n",
    "            #best_state = model.state_dict()\n",
    "            torch.save({'encoder':encoder.state_dict()},best_model_path)\n",
    "    torch.save({'encoder':encoder.state_dict()},last_model_path)\n",
    "\n",
    "    return best_val_acc,encoder\n",
    "# train_protonet(encoder,train_loader,valid_loader,conf,num_episodes_tr,num_episodes_vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "890c2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train = Datagen(conf)\n",
    "X_train,Y_train,X_val,Y_val = gen_train.generate_train()\n",
    "X_tr = torch.tensor(X_train)\n",
    "Y_tr = torch.LongTensor(Y_train)\n",
    "X_val = torch.tensor(X_val)\n",
    "Y_val = torch.LongTensor(Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71436360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf_path = '/home/asalimi/dcase-few-shot-bioacoustic/baselines/deep_learning/outputs/2022-06-06/11-10-25/Features/feat_train/Mel_train.h5'\n",
    "# hdf_train = h5py.File(hdf_path, 'r+')\n",
    "# hdf_train['features'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adba27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.to(\"cpu\")\n",
    "y = y.to(\"cpu\")\n",
    "# model = ResNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd6630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 17, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65c77b",
   "metadata": {},
   "source": [
    "# speratable transformer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64df926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is released under the CC BY-SA 4.0 license.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "# helpers\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "# classes\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, val):\n",
    "        super().__init__()\n",
    "        self.val = val\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.val\n",
    "\n",
    "\n",
    "class SepTrBlock(nn.Module):\n",
    "    def __init__(self, channels, input_size, heads=3, mlp_dim=128, dim_head=32,\n",
    "                 down_sample_input=None, project=False, reconstruct=False, dim=128, dropout_tr=0.0):\n",
    "        super().__init__()\n",
    "        patch_height, patch_width = pair(input_size)\n",
    "        self.avg_pool = nn.Identity()\n",
    "        self.upsample = nn.Identity()\n",
    "        self.projection = nn.Identity()\n",
    "        self.reconstruction = nn.Identity()\n",
    "\n",
    "        if down_sample_input is not None:\n",
    "            patch_height = patch_height // down_sample_input[0]\n",
    "            patch_width = patch_width // down_sample_input[1]\n",
    "\n",
    "            self.avg_pool = nn.AvgPool2d(kernel_size=down_sample_input)\n",
    "            self.upsample = nn.UpsamplingNearest2d(scale_factor=down_sample_input)\n",
    "\n",
    "        if project:\n",
    "            self.projection = nn.Linear(channels, dim)\n",
    "        if reconstruct:\n",
    "            self.reconstruction = nn.Sequential(\n",
    "                nn.Linear(dim, channels),\n",
    "                Scale(dim ** -0.5)\n",
    "            )\n",
    "\n",
    "        self.rearrange_patches_h = Rearrange('b c h w -> b w h c')\n",
    "        self.rearrange_patches_w = Rearrange('b c h w -> b h w c')\n",
    "\n",
    "        self.rearrange_in_tr = Rearrange('b c h w -> (b c) h w')\n",
    "        self.rearrange_out_tr_h = Rearrange('(b c) h w -> b w h c', c=patch_width)\n",
    "        self.rearrange_out_tr_w = Rearrange('(b c) h w -> b w c h', c=patch_height)\n",
    "\n",
    "        self.pos_embedding_w = nn.Parameter(torch.randn(1, 1, patch_width + 1, dim))\n",
    "        self.pos_embedding_h = nn.Parameter(torch.randn(1, 1, patch_height + 1, dim))\n",
    "        self.transformer_w = Transformer(dim, 1, heads, dim_head, mlp_dim, dropout_tr)\n",
    "        self.transformer_h = Transformer(dim, 1, heads, dim_head, mlp_dim, dropout_tr)\n",
    "\n",
    "    def forward(self, x, cls_token):\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        # H inference\n",
    "        h = self.rearrange_patches_h(x)\n",
    "        h = self.projection(h)\n",
    "\n",
    "        dim1, dim2, _, _ = h.shape\n",
    "        if cls_token.shape[0] == 1:\n",
    "            cls_token = repeat(cls_token, '() () n d -> b w n d', b=dim1, w=dim2)\n",
    "        else:\n",
    "            cls_token = repeat(cls_token, 'b () n d -> b w n d', w=dim2)\n",
    "\n",
    "        h = torch.cat((cls_token, h), dim=2)\n",
    "        h += self.pos_embedding_h\n",
    "\n",
    "        h = self.rearrange_in_tr(h)\n",
    "        h = self.transformer_h(h)\n",
    "        h = self.rearrange_out_tr_h(h)\n",
    "\n",
    "        # W inference\n",
    "        w = self.rearrange_patches_w(h[:, :, 1:, :])\n",
    "\n",
    "        cls_token = h[:, :, 0, :].unsqueeze(2)\n",
    "        cls_token = repeat(cls_token.mean((-1, -2)).unsqueeze(1).unsqueeze(1), 'b () d2 e -> b d1 d2 e', d1=w.shape[1])\n",
    "\n",
    "        w = torch.cat((cls_token, w), dim=2)\n",
    "        w += self.pos_embedding_w\n",
    "\n",
    "        w = self.rearrange_in_tr(w)\n",
    "        w = self.transformer_w(w)\n",
    "        w = self.rearrange_out_tr_w(w)\n",
    "\n",
    "        x = self.upsample(w[:, :, :, 1:])\n",
    "        x = self.reconstruction(x)\n",
    "\n",
    "        cls_token = w[:, :, :, 0].mean(2).unsqueeze(1).unsqueeze(1)\n",
    "        return x, cls_token\n",
    "\n",
    "class SeparableTr(nn.Module):\n",
    "    def __init__(self, channels=1, input_size=(128, 128), num_classes=35, depth=3, heads=5, mlp_dim=256, dim_head=256,\n",
    "                 down_sample_input=None, dim=256):\n",
    "        super().__init__()\n",
    "        inner_channels = channels\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "\n",
    "        if depth < 1:\n",
    "            raise Exception(\"Depth cannot be smaller than 1!\")\n",
    "\n",
    "        self.transformer.append(\n",
    "            SepTrBlock(channels=inner_channels, input_size=input_size, heads=heads, mlp_dim=mlp_dim,\n",
    "                       dim_head=dim_head, down_sample_input=down_sample_input, dim=dim, project=True)\n",
    "        )\n",
    "\n",
    "        for i in range(1, depth):\n",
    "            self.transformer.append(\n",
    "                SepTrBlock(channels=inner_channels, input_size=input_size, heads=heads, mlp_dim=mlp_dim,\n",
    "                           dim_head=dim_head, down_sample_input=down_sample_input, dim=dim, project=False)\n",
    "            )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, 1, dim))\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, cls_token = self.transformer[0](x, self.cls_token)\n",
    "\n",
    "        for i in range(0, len(self.transformer)):\n",
    "            x, cls_token = self.transformer[i](x, cls_token)\n",
    "\n",
    "        cls_token = cls_token[:, 0, 0, :]\n",
    "        x = self.fc(cls_token)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a6d144c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd29a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 17, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeparableTr(input_size=(17,128),dim=1)\n",
    "x.unsqueeze(-3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605fda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x.unsqueeze(-3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
